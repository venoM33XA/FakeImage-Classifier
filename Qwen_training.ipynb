{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726},{"sourceId":300340,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":256556,"modelId":277881},{"sourceId":431365,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":351632,"modelId":372879},{"sourceId":441284,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":358838,"modelId":380148},{"sourceId":443793,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":360322,"modelId":381458}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ESRGAN from here ","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/xinntao/ESRGAN","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install seedir","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n!gdown --id 1TPrz5QKd8DHHt1k8SRtm6tMiPjz_Qene","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install opencv-python glob2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seedir as sd\nsd.seedir('./', style='emoji')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, shutil\nsource = \"./RRDB_ESRGAN_x4.pth\"\ndestination = \"./ESRGAN/models/\"\nshutil.move(source,destination)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown --id 1sdB-k6-XHzN8WfAA51d8l1b7Q0TH9iBt\n!gdown --id 1JIlrwnID8-sb16DByx2vmnXIfCrFBOFb\n!gdown --id 1-dl-OtffdxzIgtYjZ8fQXkzkCNLEDSHu\n!gdown --id 1TAaSaVzf7Lfw6yT2ROuuiPSwnuCoYsRT\n!gdown --id 1v_9DWkmPVyBVR3Ax_OVepMhvmFXVsScT","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#moving these images under the LR folder\nj=0\nfor i in os.listdir(\"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/FAKE/\"):\n    if(j>100):\n        break\n       \n    source = f\"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/FAKE/{i}\"\n    destination = \"./ESRGAN/LR/\"\n    shutil.copy(source,destination)\n    j+=1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nsource = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/FAKE/1000 (10).jpg\"\ndestination = \"./ESRGAN/LR/\"\nshutil.copy(source,destination)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n%cd ESRGAN\n!python test.py     #running the ESRGAN model ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n%cd /kaggle/working/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seedir as sd\nsd.seedir('./', style='emoji')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemini_api_key ='AIzaSyCwju7w1T56sysLqfRAcbk65F1slJ_MVl4'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\nfrom PIL import Image\n\n# Load API Key (Replace with your actual API key)\ngenai.configure(api_key=gemini_api_key)\n\n# Load Gemini Vision model\nmodel = genai.GenerativeModel(\"gemini-1.5-flash\")   #load gemini ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel = timm.create_model(\"tiny_vit_21m_224.dist_in22k\", pretrained=True)\nmodel.head.fc = nn.Linear(model.head.fc.in_features, 1)\nimport os\n\nif not os.path.exists(\"/kaggle/input/tiny_vit_4000/pytorch/default/1/tiny_vit_modified.pth\"):\n    print(\"Error: model2.pth not found!\")\n\nmodel.load_state_dict(torch.load(\"/kaggle/input/tiny_vit_4000/pytorch/default/1/tiny_vit_modified.pth\"))\nmodel = model.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_config = timm.data.resolve_model_data_config(model)\ntest_transforms = timm.data.create_transform(**data_config, is_training=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_config = timm.data.resolve_model_data_config(model)\ntest_transforms = timm.data.create_transform(**data_config, is_training=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GradCAM:\n    def __init__(self, model, target_layer_name):\n        self.model = model\n        self.target_layer_name = target_layer_name\n        self.gradients = None\n        self.activations = None\n        self._register_hooks()\n\n    def _register_hooks(self):\n        target_layer = dict(self.model.named_modules())[self.target_layer_name]\n\n        def forward_hook(module, input, output):\n            self.activations = output\n\n        def backward_hook(module, grad_input, grad_output):\n            self.gradients = grad_output[0]\n\n        target_layer.register_forward_hook(forward_hook)\n        target_layer.register_full_backward_hook(backward_hook)\n\n    def generate(self, input_tensor, class_idx=None):\n        self.model.eval()\n        output = self.model(input_tensor)\n\n        if class_idx is None:\n            class_idx = output.argmax(dim=1).item()\n\n        # Backpropagate\n        self.model.zero_grad()\n        output[:, class_idx].backward(retain_graph=True)\n\n        # Compute Grad-CAM\n        gradients = self.gradients.cpu().detach()\n        activations = self.activations.cpu().detach()\n\n        weights = gradients.mean(dim=(2, 3), keepdim=True)\n        grad_cam = torch.sum(weights * activations, dim=1).squeeze()\n        grad_cam = F.relu(grad_cam)\n\n        # Normalize the Grad-CAM output\n        grad_cam -= grad_cam.min()\n        grad_cam /= grad_cam.max()\n        return grad_cam.numpy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef gradcam(image, grad_cam, alpha=0.5, cmap=\"jet\"):\n    grad_cam_resized = (\n        F.interpolate(\n            torch.tensor(grad_cam).unsqueeze(0).unsqueeze(0),\n            size=image.shape[:2],\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n        .squeeze()\n        .numpy()\n    )\n\n    heatmap = plt.get_cmap(cmap)(grad_cam_resized)[..., :3]\n    heatmap = (heatmap * 255).astype(np.uint8)\n\n    return heatmap\n\n\n# Grad-CAM for high-level feature map\ntarget_layer = \"stages.3.blocks.1.local_conv\"\ngrad_cam = GradCAM(model, target_layer)                   #tiny_vit predictions and gradcam output ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef merge_original_and_gradcam(\n    original_img_path, gradcam_output, alpha=0.5, cmap=\"jet\"\n):\n    original_img = cv2.imread(original_img_path)\n    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n\n    gradcam_resized = cv2.resize(\n        gradcam_output, (original_img.shape[1], original_img.shape[0])\n    )\n\n    gradcam_resized = (gradcam_resized * 255).astype(np.uint8)\n\n    heatmap = cv2.applyColorMap(gradcam_resized, cv2.COLORMAP_JET)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n\n    overlay = cv2.addWeighted(original_img, 1 - alpha, heatmap, alpha, 0)\n    return overlay\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" dir_path = \"/kaggle/working/gradcam_images\"\ni=0\n# Create the directory if it doesn't exist\nos.makedirs(dir_path, exist_ok=True)\nfor img in tqdm(os.listdir(\"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/FAKE\")):\n  if(i<=100):\n    img_path = os.path.join(\"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/FAKE\", img)\n    image = Image.open(img_path).convert(\"RGB\")\n    image = test_transforms(image).unsqueeze(0).to(device)\n\n    grad_cam_output = grad_cam.generate(image)\n    image = plt.imread(img_path)\n    grad_cam_output = gradcam(image, grad_cam_output)\n    overlay = merge_original_and_gradcam(img_path, grad_cam_output)\n\n    plt.imsave(f\"gradcam_images/{img}\", overlay)\n    i+=1\n  else:\n      break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"\"\" Analyze the provided image ,a model trained to detect fake visuals detected it as FAKE . Identify and explain artifacts that indicate it is fake. Focus primarily on the original image to identify and explain distinguishing artifacts that indicate it is fake. Use the Grad-CAM output for reference only when necessary. Provide clear, concise explanations (maximum 50 words each) using the specified artifacts below. Include positional references like 'top left' or 'bottom right' when relevant. DO NOT include any other sentences or artifacts in your response. Select only 6-7 relevant artifacts.\nmake prediction of the category of the image ,verify from the artifacts below \ncategories = {\n    \"airplane\": [\n        \"Artificial noise patterns in uniform surfaces\",\n        \"Metallic surface artifacts\",\n        \"Impossible mechanical connections\",\n        \"Inconsistent scale of mechanical parts\",\n        \"Physically impossible structural elements\",\n        \"Implausible aerodynamic structures\",\n        \"Misaligned body panels\",\n        \"Impossible mechanical joints\",\n        \"Distorted window reflections\",\n    ],\n    \"automobile\": [\n        \"Artificial noise patterns in uniform surfaces\",\n        \"Metallic surface artifacts\",\n        \"Impossible mechanical connections\",\n        \"Inconsistent scale of mechanical parts\",\n        \"Physically impossible structural elements\",\n        \"Incorrect wheel geometry\",\n        \"Misaligned body panels\",\n        \"Impossible mechanical joints\",\n        \"Distorted window reflections\",\n    ],\n    \"ship\": [\n        \"Artificial noise patterns in uniform surfaces\",\n        \"Metallic surface artifacts\",\n        \"Impossible mechanical connections\",\n        \"Inconsistent scale of mechanical parts\",\n        \"Physically impossible structural elements\",\n        \"Misaligned body panels\",\n    ],\n    \"truck\": [\n        \"Artificial noise patterns in uniform surfaces\",\n        \"Metallic surface artifacts\",\n        \"Impossible mechanical connections\",\n        \"Inconsistent scale of mechanical parts\",\n        \"Physically impossible structural elements\",\n        \"Incorrect wheel geometry\",\n        \"Misaligned body panels\",\n        \"Impossible mechanical joints\",\n        \"Distorted window reflections\",\n    ],\n    \"bird\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n    ],\n    \"cat\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n        \"Anatomically incorrect paw structures\",\n        \"Improper fur direction flows\",\n    ],\n    \"deer\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n        \"Improper fur direction flows\",\n    ],\n    \"dog\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n        \"Dental anomalies in mammals\",\n        \"Anatomically incorrect paw structures\",\n        \"Improper fur direction flows\",\n    ],\n    \"frog\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n    ],\n    \"horse\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n        \"Dental anomalies in mammals\",\n    ],\n    \"major\": [\n        \"Discontinuous surfaces\",\n        \"Non-manifold geometries in rigid structures\",\n        \"Asymmetric features in naturally symmetric objects\",\n        \"Texture bleeding between adjacent regions\",\n        \"Excessive sharpness in certain image regions\",\n        \"Artificial smoothness\",\n        \"Movie-poster-like composition of ordinary scenes\",\n        \"Unnatural lighting gradients\",\n        \"Fake depth of field\",\n        \"Abruptly cut-off objects\",\n        \"Color coherence breaks\",\n        \"Spatial relationship errors\",\n        \"Depth perception anomalies\",\n        \"Over-sharpening artifacts\",\n        \"Incorrect reflection mapping\",\n        \"Inconsistent object boundaries\",\n        \"Floating or disconnected components\",\n        \"Texture repetition patterns\",\n        \"Unrealistic specular highlights\",\n        \"Inconsistent material properties\",\n        \"Inconsistent shadow directions\",\n        \"Multiple light source conflicts\",\n        \"Missing ambient occlusion\",\n        \"Incorrect perspective rendering\",\n        \"Scale inconsistencies within single objects\",\n        \"Aliasing along high-contrast edges\",\n        \"Blurred boundaries in fine details\",\n        \"Jagged edges in curved structures\",\n        \"Random noise patterns in detailed areas\",\n        \"Loss of fine detail in complex structures\",\n        \"Artificial enhancement artifacts\",\n        \"Repeated element patterns\",\n        \"Systematic color distribution anomalies\",\n        \"Frequency domain signatures\",\n        \"Unnatural color transitions\",\n        \"Resolution inconsistencies within regions\",\n        \"Glow or light bleed around object boundaries\",\n        \"Ghosting effects: Semi-transparent duplicates of elements\",\n        \"Cinematization effects\",\n        \"Dramatic lighting that defies natural physics\",\n        \"Artificial depth of field in object presentation\",\n        \"Unnaturally glossy surfaces\",\n        \"Synthetic material appearance\",\n        \"Multiple inconsistent shadow sources\",\n        \"Exaggerated characteristic features\",\n        \"Scale inconsistencies within the same object class\",\n        \"Incorrect skin tones\",\n    ],\n}\nOutput Format:\nWrite each artifact and explanation on a separate line, using the format:\nArtifact Name: Explanation.\nFor example:\n\nNotes:\nExplanations should remain under 50 words for clarity.\nAVOID referencing artifacts not listed or including extra commentary.\n\nChoose from the below list depending on the image:\n\n\"\"\"  \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai\nfrom PIL import Image\nimport os\nimport glob\nimport re  # To remove \"_rlt\"\nimport time \n\nmodel = genai.GenerativeModel(\"gemini-1.5-pro\")\n\n# Define paths\nesrgan_folder = \"/kaggle/working/ESRGAN/results/\"\ngradcam_folder = \"/kaggle/working/gradcam_images/\"\noutput_folder = \"/kaggle/working/gemini_outputs_v2/\"  # NEW folder\n\n# Ensure output folder exists\nos.makedirs(output_folder, exist_ok=True)\n\n# Get all ESRGAN image paths\nesrgan_images = glob.glob(os.path.join(esrgan_folder, \"*.png\"))  # Adjust extension if needed\n\nprocessed_count = 0\nmax_pairs = 100  # Process only 10 image pairs\n\n# Process only ESRGAN images that have a matching Grad-CAM image\nfor esrgan_path in esrgan_images:\n    if processed_count >= max_pairs:\n        break\n\n    filename = os.path.basename(esrgan_path)  # e.g., \"4599 (6)_rlt.png\"\n    base_name = os.path.splitext(filename)[0]  # Remove .png, e.g., \"4599 (6)_rlt\"\n\n    # Remove \"_rlt\" if present\n    clean_name = re.sub(r\"_rlt$\", \"\", base_name)  # e.g., \"4599 (6)\" remove extensions \n\n    # Find corresponding Grad-CAM image\n    gradcam_path = os.path.join(gradcam_folder, f\"{clean_name}.jpg\")  # Adjust extension if needed\n\n    # Skip ESRGAN image if no matching Grad-CAM image\n    if not os.path.exists(gradcam_path):   #skip demo images \n        print(f\"Skipping {filename}: No corresponding Grad-CAM image found.\")\n        continue\n\n    # Load images\n    image1 = Image.open(esrgan_path)\n    image2 = Image.open(gradcam_path)\n\n    \n    response = model.generate_content([image1, image2, prompt])\n\n   \n    output_path = os.path.join(output_folder, f\"{clean_name}.txt\")  #save output \n    with open(output_path, \"w\") as f:\n        f.write(response.text)\n\n    print(f\"Processed {filename} -> Output saved to {output_path}\")\n    processed_count += 1\n    time.sleep(7)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_structured_messages(messages, image_paths):\n    structured_messages = []\n\n    for msg in messages:\n        role = msg[\"role\"]\n        content = msg[\"content\"]\n\n        if role == \"user\":\n            parts = content.split(\"<image>\")\n            content_list = []\n\n            # Add image parts in order\n            for i in range(content.count(\"<image>\")):\n                content_list.append({\n                    \"type\": \"image\",\n                    \"image\": f\"file://{image_paths[i]}\"\n                })\n\n            # Add remaining text (if any)\n            remaining_text = parts[-1].strip()\n            if remaining_text:\n                content_list.append({\n                    \"type\": \"text\",\n                    \"text\": remaining_text\n                })\n\n            structured_messages.append({\n                \"role\": \"user\",\n                \"content\": content_list\n            })\n\n        else:\n            # For 'system' or 'assistant' roles, keep as simple text\n            structured_messages.append({\n                \"role\": role,\n                \"content\": msg[\"content\"]\n            })\n\n    return structured_messages\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os\nimport json\nimport base64\nimport re\n\n# Define directories\nimages1_dir = \"/kaggle/working/ESRGAN/results\"\nimages2_dir = \"/kaggle/working/gradcam_images\"\ntexts_dir = \"/kaggle/working/gemini_outputs_v2\"\noutput_file = \"/kaggle/working/dataset_qwen.jsonl\"\n\n# Encode image to base64 (optional, if needed)\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        encoded = base64.b64encode(image_file.read()).decode(\"utf-8\")\n        ext = os.path.splitext(image_path)[1].lower()\n        mime = \"image/png\" if ext == \".png\" else \"image/jpeg\"\n        return f\"data:{mime};base64,{encoded}\"\n\n# Clean filename to match\ndef clean_filename(filename):\n    filename = os.path.splitext(filename)[0]\n    return re.sub(r\"_rlt$\", \"\", filename)\n\n# Process\ndataset = []\nimage_filenames = sorted(os.listdir(images1_dir))\n\nfor filename in image_filenames:\n    base_name = clean_filename(filename)\n\n    img1_path = os.path.join(images1_dir, filename)\n    img2_path = os.path.join(images2_dir, base_name + \".jpg\")\n    text_path = os.path.join(texts_dir, base_name + \".txt\")\n\n    if not (os.path.exists(img1_path) and os.path.exists(img2_path) and os.path.exists(text_path)):\n        print(f\"Skipping: {base_name} due to missing file\")\n        continue\n\n    with open(text_path, \"r\", encoding=\"utf-8\") as file:\n        label = file.read().strip()\n\n    # Construct prompt text (customize this based on your use case)\n   # prompt = \"Please analyze the differences between the two images and explain your reasoning.\"\n\n    # Format for Qwen\n    entry = {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant, who can classify real and fake images and clearly produce different outputs instead of just copying the input.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"<image>\\n<image>\\n{prompt}\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": label+ \"<|endoftext|>\"\n            }\n        ],\n        \"images\": [img1_path, img2_path]\n    }\n    structured=convert_to_structured_messages(entry[\"messages\"],entry[\"images\"])\n    dataset.append(structured)\n\n# Write to JSONL\nwith open(output_file, \"w\", encoding=\"utf-8\") as f:\n    for entry in dataset:\n        f.write(json.dumps(entry) + \"\\n\")\n\nprint(f\"✅ Dataset for Qwen saved to {output_file}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install  datasets torch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y accelerate\n!pip uninstall -y accelerate  # run twice to be sure\n!pip uninstall -y transformers peft\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --no-cache-dir accelerate transformers peft\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip show transformers ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# data json prep till here ","metadata":{}},{"cell_type":"code","source":"import base64\nfrom PIL import Image\nfrom io import BytesIO\n\ndef decode_base64_to_image(b64_string):\n    # Handle possible data URI format\n    if b64_string.startswith(\"data:image\"):\n        b64_string = b64_string.split(\",\", 1)[1]\n    image_data = base64.b64decode(b64_string)\n    return Image.open(BytesIO(image_data)).convert(\"RGB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# model config ","metadata":{}},{"cell_type":"code","source":"pip install transformers_stream_generator\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoTokenizer, AutoModelForVision2Seq\nimport torch\n\nmodel_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n\n# Load the tokenizer and processor\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n\n# Load the vision-language model\nmodel = AutoModelForVision2Seq.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    device_map=\"auto\",  # or 'cuda:0' if you're only using one GPU\n    torch_dtype=torch.float16\n)\n\n# ✅ Ensure pad_token is set\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = \"<|endoftext|>\"\n\n# ✅ Print pad token info\nprint(\"PAD Token:\", tokenizer.pad_token)\nprint(\"PAD Token ID:\", tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(model)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom peft.utils import prepare_model_for_kbit_training\n\n# ✅ Define target LoRA modules based on Qwen architecture\ntarget_modules = [\n    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n    \"fc_in\", \"fc_out\",  # These are typical for Qwen linear blocks\n]\n\n# ✅ LoRA Configuration\nlora_config = LoraConfig(\n    r=8,  # Rank\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",  # or TaskType.CAUSAL_LM\n    target_modules=target_modules\n    \n)\n\n# ✅ Prepare model for LoRA\n# Optional: if you're using 8-bit/4-bit quantization, uncomment this\n# model = prepare_model_for_kbit_training(model)\n\n# ✅ Apply LoRA\nmodel = get_peft_model(model, lora_config)\n\n# ✅ Set model to training mode\nmodel.train()\n\n# ✅ Print trainable parameters (sanity check)\n\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# data prep for training","metadata":{}},{"cell_type":"code","source":"print(dataset[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(processor)\nmodel.device ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install qwen_vl_utils\nfrom qwen_vl_utils import process_vision_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#testing a sample input for finetuning \nsample = dataset[0]\n\n# 1. Build prompt from chat\ntext = processor.apply_chat_template(\n    sample, tokenize=False, add_generation_prompt=True\n)\nprint(\"printing text: \")\nprint(text)\n\n# 2. Load the two images\nimage_inputs, video_inputs = process_vision_info(\n    sample\n)\nprint(\"image_inputs\")\nprint(image_inputs)\nprint(\"done\")\ninputs = processor(\n    text=text,\n    images=image_inputs,\n    videos=None,\n    padding=True,\n    return_tensors=\"pt\",\n)\n\nprint(inputs)\ninputs = inputs.to(\"cuda\")\n\nprint(\"Generating text from the model...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\nprint(f\"Generated IDs: {generated_ids}\")\n\n# Trimming the generated IDs to exclude the input tokens\nprint(\"Trimming the generated IDs to exclude the input tokens...\")\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\nprint(f\"Trimmed generated IDs: {generated_ids_trimmed}\")\n\n# Decode the generated IDs into text\nprint(\"Decoding the generated IDs to text...\")\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(f\"Decoded output text: {output_text}\")\n\n# Print final output\nprint(\"Final output text:\")\nfor text in output_text:\n    print(text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\nfrom qwen_vl_utils import process_vision_info\n\nclass QwenVLDataset(Dataset):\n    def __init__(self, dataset, processor):\n        self.dataset = dataset  # List of dicts with 'conversations' key\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        sample = self.dataset[idx]\n        messages = sample  # Already in structured format\n\n        # 1. Generate prompt from structured messages\n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n        # 2. Extract visual inputs from messages (file:///... assumed valid)\n        image_inputs, video_inputs = process_vision_info(messages)\n\n        # 3. Tokenize text + attach image features\n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            return_tensors=\"pt\",\n            padding=\"longest\"\n        )\n\n        input_ids = inputs[\"input_ids\"][0]\n        attention_mask = inputs[\"attention_mask\"][0]\n\n        # 4. Extract assistant target reply\n        assistant_msg = next((m for m in reversed(messages) if m[\"role\"] == \"assistant\"), None)\n        target_text = assistant_msg[\"content\"] if assistant_msg else \"\"\n\n        target_token_ids = self.processor.tokenizer(\n            target_text, return_tensors=\"pt\", add_special_tokens=False\n        )[\"input_ids\"][0]\n\n        # 5. Mask non-target tokens\n        labels = torch.full_like(input_ids, -100)\n        if len(target_token_ids) > 0 and len(target_token_ids) <= len(input_ids):\n            labels[-len(target_token_ids):] = target_token_ids\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = QwenVLDataset(dataset, processor)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'].shape)\nprint(train_dataset[3]['input_ids'].shape)\nprint(train_dataset[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataset)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom transformers import default_data_collator\n\ndata_collator = default_data_collator\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen-lora\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    save_strategy=\"steps\",\n    save_steps=100,\n    num_train_epochs=7,\n    learning_rate=2e-5,\n    fp16=True,\n    report_to=\"none\"\n)\n\n# Dataset should return: {\"messages\": [...], \"images\": [...]}\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator\n)\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = processor.apply_chat_template(\n    dataset[0], tokenize=False, add_generation_prompt=True\n)\nfrom qwen_vl_utils import process_vision_info\nimage_inputs, video_inputs = process_vision_info(dataset[0])\ninputs = processor(\n    text=text,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\nprint(\"Generating text from the model...\")\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\nprint(f\"Generated IDs: {generated_ids}\")\n\n# Trimming the generated IDs to exclude the input tokens\nprint(\"Trimming the generated IDs to exclude the input tokens...\")\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\nprint(f\"Trimmed generated IDs: {generated_ids_trimmed}\")\n\n# Decode the generated IDs into text\nprint(\"Decoding the generated IDs to text...\")\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(f\"Decoded output text: {output_text}\")\n\n# Print final output\nprint(\"Final output text:\")\nfor text in output_text:\n    print(text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/qwen-finetuned\")\ntokenizer.save_pretrained(\"/kaggle/working/qwen-finetuned\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('/kaggle/working/myqwenfinal', 'zip', '/kaggle/working/qwen-finetuned')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer,AutoModelForVision2Seq\n\nmodel = AutoModelForVision2Seq.from_pretrained(\"/kaggle/working/qwen-finetuned\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/qwen-finetuned\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.device ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = processor.apply_chat_template(\n    dataset[1], tokenize=False, add_generation_prompt=True\n)\nfrom qwen_vl_utils import process_vision_info\nimage_inputs, video_inputs = process_vision_info(dataset[1])\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\nprint(\"Generating text from the model...\")\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\nprint(f\"Generated IDs: {generated_ids}\")\n\n# Trimming the generated IDs to exclude the input tokens\nprint(\"Trimming the generated IDs to exclude the input tokens...\")\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\nprint(f\"Trimmed generated IDs: {generated_ids_trimmed}\")\n\n# Decode the generated IDs into text\nprint(\"Decoding the generated IDs to text...\")\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(f\"Decoded output text: {output_text}\")\n\n# Print final output\nprint(\"Final output text:\")\nfor text in output_text:\n    print(text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom qwen_vl_utils import process_vision_info\n\n# Prepare the input text using chat template\ntext = processor.apply_chat_template(\n    dataset[0][\"messages\"], tokenize=False, add_generation_prompt=True\n)\n\n# Extract image and video inputs from the message\nimage_inputs, video_inputs = process_vision_info(dataset[0][\"messages\"])\n\n# Tokenize and convert to tensors\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\n\n# Move tensors to GPU\ninputs = inputs.to(\"cuda\")\n\n# Generate predictions\nprint(\"Generating text from the model...\")\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\nprint(f\"Generated IDs: {generated_ids}\")\n\n# Trim generated tokens to exclude input tokens\nprint(\"Trimming the generated IDs to exclude the input tokens...\")\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\nprint(f\"Trimmed generated IDs: {generated_ids_trimmed}\")\n\n# Decode the output tokens to text\nprint(\"Decoding the generated IDs to text...\")\noutput_text = processor.batch_decode(\n    generated_ids_trimmed,\n    skip_special_tokens=True,\n    clean_up_tokenization_spaces=False,\n)\nprint(\"Final output text:\")\n\n# Display output text and corresponding images\nfor idx, text in enumerate(output_text):\n    print(f\"\\n--- Output {idx + 1} ---\")\n    print(text)\n\n    # Display corresponding image if available\n    if image_inputs and idx < len(image_inputs):\n        image = image_inputs[idx]\n\n        # Convert tensor to PIL image if needed\n        if isinstance(image, torch.Tensor):\n            # Assume shape: (C, H, W); normalize values to 0-1\n            image = image.detach().cpu()\n            image = image.permute(1, 2, 0).clamp(0, 1).numpy()\n            plt.imshow(image)\n        elif isinstance(image, Image.Image):\n            plt.imshow(image)\n        else:\n            print(f\"Unsupported image format for index {idx}\")\n            continue\n\n        plt.axis(\"off\")\n        plt.title(f\"Image for Output {idx + 1}\")\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_dir = \"/kaggle/working/model\"\n\n# Create the save directory\nos.makedirs(save_dir, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Saving model to /kaggle/working/model ...\")\ntokenizer.save_pretrained(save_dir)\nprocessor.save_pretrained(save_dir)\nmodel.save_pretrained(save_dir)\nprint(\"✅ Done: Model saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Zip the /kaggle/working/model folder into /kaggle/working/qwen_model.zip\nshutil.make_archive(\"/kaggle/working/qwen_model\", 'zip', \"/kaggle/working/model\")\n\nprint(\"✅ Model zipped as qwen_model.zip\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''import shutil\n\n# This will create a zip file at /kaggle/working/working_dir.zip\nshutil.make_archive(\"/kaggle/working/working_dir\", 'zip', \"/kaggle/working\")\n\nprint(\"✅ Entire /kaggle/working directory zipped as working_dir.zip\")\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"INFERENCE ****","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForVision2Seq\nfrom peft import PeftModel\n\n# Load base model\nbase_model = AutoModelForVision2Seq.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\",  # Use the exact base model you adapted from\n    device_map=\"auto\",   # optional for multi-GPU or accelerator support\n    trust_remote_code=True  # Qwen may use custom model classes\n)\n\n# Load tokenizer from your adapter directory\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", trust_remote_code=True)\n\n# Load adapter on top of base model\nmodel = PeftModel.from_pretrained(base_model, \"/kaggle/input/qwen_semifinal/pytorch/default/1\")\nmodel.eval()\n\nmodel.eval()\n\nprint(\"✅ Adapter model loaded successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoProcessor \nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", trust_remote_code=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel1= timm.create_model(\"tiny_vit_21m_224.dist_in22k\", pretrained=True)\nmodel1.head.fc = nn.Linear(model1.head.fc.in_features, 1)\nimport os\n\nif not os.path.exists(\"/kaggle/input/tiny_vit_4000/pytorch/default/1/tiny_vit_modified.pth\"):\n    print(\"Error: model2.pth not found!\")\n\nmodel1.load_state_dict(torch.load(\"/kaggle/input/tiny_vit_4000/pytorch/default/1/tiny_vit_modified.pth\"))\nmodel1 = model1.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_config = timm.data.resolve_model_data_config(model1)\ntest_transforms = timm.data.create_transform(**data_config, is_training=False)\n# 1. Load the model\nmodel1.eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device) \n# 2. Load and transform the image\nimg_path = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/FAKE/1012 (5).jpg\"\nimage = Image.open(img_path).convert(\"RGB\")\nimage = test_transforms(image).unsqueeze(0).to(device)  # Add batch dimension\nprint(device)\n# 3. Run inference\nwith torch.no_grad():\n    output = model1(image).view(-1, 1)        # shape: [1, 1]\n    prob = torch.sigmoid(output)             # Convert logits to probability\n    pred = (prob > 0.5).float().item()       # Threshold at 0.5\n\nprint(f\"Probability of being real: {prob.item():.4f}\")\nprint(f\"Predicted class: {'FAKE' if pred == 0.0 else 'REAL'}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load the image\n#img_path = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test/REAL/0005 (8).jpg\"\nimage = Image.open(img_path).convert(\"RGB\")\n\n# Display the image\nplt.imshow(image)\nplt.axis(\"off\")\nplt.title(\"REAL - 0005 (8).jpg\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.images = glob.glob(f\"{root_dir}/REAL/*.jpg\") + glob.glob(\n            f\"{root_dir}/FAKE/*.jpg\"\n        )\n        self.labels = [1] * len(glob.glob(f\"{root_dir}/REAL/*.jpg\")) + [0] * len(\n            glob.glob(f\"{root_dir}/FAKE/*.jpg\")\n        )\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        img = Image.open(img_path)\n        if self.transform:\n            img = self.transform(img)\n        label = self.labels[idx]\n        return img, torch.tensor(label, dtype=torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATASET_PATH =' /kaggle/input/test ' # Update if needed\nREAL_PATH = os.path.join(DATASET_PATH, \"real\")\nFAKE_PATH = os.path.join(DATASET_PATH, \"fake\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob \ntest_ds = CustomDataset(root_dir=\"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test\", transform=test_transforms)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader \ntest_loader = DataLoader(\n    test_ds, batch_size=32, shuffle=True, num_workers=4, pin_memory=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Ensure model is on correct device\nmodel1.to(device)\nmodel1.eval()\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        labels = labels.view(-1, 1).float().to(device)\n        images = images.to(device)\n\n        outputs = model1(images).view(-1, 1)\n        probs = torch.sigmoid(outputs)\n        preds = (probs > 0.5).int()\n\n        all_preds.extend(preds.cpu().numpy().flatten())\n        all_labels.extend(labels.cpu().numpy().flatten())\n\n# Compute metrics\naccuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds)\nrecall = recall_score(all_labels, all_preds)\nf1 = f1_score(all_labels, all_preds)\n\nprint(f\"Accuracy : {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(all_preds[:100])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(all_labels[:100])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nsource = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test/FAKE/10 (5).jpg\"\ndestination = \"./ESRGAN/LR/\"\nshutil.copy(source,destination)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd ESRGAN\n!python test.py     #running the ESRGAN model \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load the image\nimg_path = \"/kaggle/working/ESRGAN/results/10 (5)_rlt.png\"\nimage = Image.open(img_path).convert(\"RGB\")\n\n# Display the image\nplt.imshow(image)\nplt.axis(\"off\")\nplt.title(\"REAL - 100 (4).jpg\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprompt = \"\"\" Analyze the provided image ,a model trained to detect fake visuals detected it as FAKE . Identify and explain artifacts that indicate it is fake. Focus primarily on the original image to identify and explain distinguishing artifacts that indicate it is fake. Use the Grad-CAM output for reference only when necessary. Provide clear, concise explanations (maximum 50 words each) using the specified artifacts below. Include positional references like 'top left' or 'bottom right' when relevant. DO NOT include any other sentences or artifacts in your response. Select only 6-7 relevant artifacts.\nmake prediction of the category of the image ,verify from the artifacts below \ncategories = {\n    \"airplane\": [\n        \"Artificial noise patterns in uniform surfaces\",\n        \"Metallic surface artifacts\",\n        \"Impossible mechanical connections\",\n        \"Inconsistent scale of mechanical parts\",\n        \"Physically impossible structural elements\",\n        \"Implausible aerodynamic structures\",\n        \"Misaligned body panels\",\n        \"Impossible mechanical joints\",\n        \"Distorted window reflections\",\n    ],\n    \"automobile\": [\n        \"Artificial noise patterns in uniform surfaces\",\n        \"Metallic surface artifacts\",\n        \"Impossible mechanical connections\",\n        \"Inconsistent scale of mechanical parts\",\n        \"Physically impossible structural elements\",\n        \"Incorrect wheel geometry\",\n        \"Misaligned body panels\",\n        \"Impossible mechanical joints\",\n        \"Distorted window reflections\",\n    ],\n    \"ship\": [\n        \"Artificial noise patterns in uniform surfaces\",\n        \"Metallic surface artifacts\",\n        \"Impossible mechanical connections\",\n        \"Inconsistent scale of mechanical parts\",\n        \"Physically impossible structural elements\",\n        \"Misaligned body panels\",\n    ],\n    \"truck\": [\n        \"Artificial noise patterns in uniform surfaces\",\n        \"Metallic surface artifacts\",\n        \"Impossible mechanical connections\",\n        \"Inconsistent scale of mechanical parts\",\n        \"Physically impossible structural elements\",\n        \"Incorrect wheel geometry\",\n        \"Misaligned body panels\",\n        \"Impossible mechanical joints\",\n        \"Distorted window reflections\",\n    ],\n    \"bird\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n    ],\n    \"cat\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n        \"Anatomically incorrect paw structures\",\n        \"Improper fur direction flows\",\n    ],\n    \"deer\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n        \"Improper fur direction flows\",\n    ],\n    \"dog\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n        \"Dental anomalies in mammals\",\n        \"Anatomically incorrect paw structures\",\n        \"Improper fur direction flows\",\n    ],\n    \"frog\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n    ],\n    \"horse\": [\n        \"Unrealistic eye reflections\",\n        \"Misshapen ears or appendages\",\n        \"Anatomically impossible joint configurations\",\n        \"Unnatural pose artifacts\",\n        \"Biological asymmetry errors\",\n        \"Regular grid-like artifacts in textures\",\n        \"Impossible foreshortening in animal bodies\",\n        \"Misaligned bilateral elements in animal faces\",\n        \"Over-smoothing of natural textures\",\n        \"Dental anomalies in mammals\",\n    ],\n    \"major\": [\n        \"Discontinuous surfaces\",\n        \"Non-manifold geometries in rigid structures\",\n        \"Asymmetric features in naturally symmetric objects\",\n        \"Texture bleeding between adjacent regions\",\n        \"Excessive sharpness in certain image regions\",\n        \"Artificial smoothness\",\n        \"Movie-poster-like composition of ordinary scenes\",\n        \"Unnatural lighting gradients\",\n        \"Fake depth of field\",\n        \"Abruptly cut-off objects\",\n        \"Color coherence breaks\",\n        \"Spatial relationship errors\",\n        \"Depth perception anomalies\",\n        \"Over-sharpening artifacts\",\n        \"Incorrect reflection mapping\",\n        \"Inconsistent object boundaries\",\n        \"Floating or disconnected components\",\n        \"Texture repetition patterns\",\n        \"Unrealistic specular highlights\",\n        \"Inconsistent material properties\",\n        \"Inconsistent shadow directions\",\n        \"Multiple light source conflicts\",\n        \"Missing ambient occlusion\",\n        \"Incorrect perspective rendering\",\n        \"Scale inconsistencies within single objects\",\n        \"Aliasing along high-contrast edges\",\n        \"Blurred boundaries in fine details\",\n        \"Jagged edges in curved structures\",\n        \"Random noise patterns in detailed areas\",\n        \"Loss of fine detail in complex structures\",\n        \"Artificial enhancement artifacts\",\n        \"Repeated element patterns\",\n        \"Systematic color distribution anomalies\",\n        \"Frequency domain signatures\",\n        \"Unnatural color transitions\",\n        \"Resolution inconsistencies within regions\",\n        \"Glow or light bleed around object boundaries\",\n        \"Ghosting effects: Semi-transparent duplicates of elements\",\n        \"Cinematization effects\",\n        \"Dramatic lighting that defies natural physics\",\n        \"Artificial depth of field in object presentation\",\n        \"Unnaturally glossy surfaces\",\n        \"Synthetic material appearance\",\n        \"Multiple inconsistent shadow sources\",\n        \"Exaggerated characteristic features\",\n        \"Scale inconsistencies within the same object class\",\n        \"Incorrect skin tones\",\n    ],\n}\nOutput Format:\nWrite each artifact and explanation on a separate line, using the format:\nArtifact Name: Explanation.\nFor example:\n\nNotes:\nExplanations should remain under 50 words for clarity.\nAVOID referencing artifacts not listed or including extra commentary.\n\nChoose from the below list depending on the image:\n\n\"\"\"  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoTokenizer, AutoModelForVision2Seq\nimport torch\n\nmodel_name = \"Qwen/Qwen2-VL-2B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n\n# Load the vision-language model\nmodel2 = AutoModelForVision2Seq.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    device_map=\"auto\",  # or 'cuda:0' if you're only using one GPU\n    torch_dtype=torch.float16\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install qwen_vl_utils --quiet ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_structured_messages(messages, image_paths):\n    structured_messages = []\n\n    for msg in messages:\n        role = msg[\"role\"]\n        content = msg[\"content\"]\n\n        if role == \"user\":\n            parts = content.split(\"<image>\")\n            content_list = []\n\n            # Add image parts in order\n            for i in range(content.count(\"<image>\")):\n                content_list.append({\n                    \"type\": \"image\",\n                    \"image\": f\"file://{image_paths[i]}\"\n                })\n\n            # Add remaining text (if any)\n            remaining_text = parts[-1].strip()\n            if remaining_text:\n                content_list.append({\n                    \"type\": \"text\",\n                    \"text\": remaining_text\n                })\n\n            structured_messages.append({\n                \"role\": \"user\",\n                \"content\": content_list\n            })\n\n        else:\n            # For 'system' or 'assistant' roles, keep as simple text\n            structured_messages.append({\n                \"role\": role,\n                \"content\": msg[\"content\"]\n            })\n\n    return structured_messages\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.device","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoModelForVision2Seq\nfrom PIL import Image\nimport torch\nfrom qwen_vl_utils import process_vision_info\n\n# === Load model and processor (on GPU) ===\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# === Load image ===\n # Replace with your actual image pat\n\n# === Prompt ===\n\n# === Build chat message ===\nentry = {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant, who can classify real and fake images and clearly produce different outputs instead of just copying the input.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"<image>\\n{prompt}\"\n            }\n        ],\n        \"images\": [img_path]\n    }\nmessages=convert_to_structured_messages(entry[\"messages\"],entry[\"images\"])\n# === Apply chat template and prepare inputs ===\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nimage_inputs, video_inputs = process_vision_info(messages)  # 👈 pass image here\n\ninputs = processor(\n    text=text,\n    images=image_inputs,\n    videos=video_inputs,\n    return_tensors=\"pt\",\n    padding=True,\n)\n\n# === Move inputs to GPU and cast to long ===\ninputs = {k: v.to(device) for k, v in inputs.items()}\ninputs[\"input_ids\"] = inputs[\"input_ids\"].long()\nif \"attention_mask\" in inputs:\n    inputs[\"attention_mask\"] = inputs[\"attention_mask\"].long()\n\n# === Inference ===\nwith torch.no_grad():\n    print(\"🧠 Generating explanation...\")\n    generated_ids = model.generate(**inputs, max_new_tokens=256)\n\n# === Decode output ===\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs[\"input_ids\"], generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\n\n# === Final Output ===\nprint(\"📢 Final Output:\")\nprint(output_text[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages.device","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoModelForVision2Seq\nfrom PIL import Image\nimport torch\nfrom qwen_vl_utils import process_vision_info\n\n# === Load model and processor (on GPU) ===\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# === Load image ===\n # Replace with your actual image pat\n\n# === Prompt ===\n\n# === Build chat message ===\nentry = {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant, who can classify real and fake images and clearly produce different outputs instead of just copying the input.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"<image>\\n{prompt}\"\n            }\n        ],\n        \"images\": [img_path]\n    }\nmessages=convert_to_structured_messages(entry[\"messages\"],entry[\"images\"])\n# === Apply chat template and prepare inputs ===\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nimage_inputs, video_inputs = process_vision_info(messages)  # 👈 pass image here\n\ninputs = processor(\n    text=text,\n    images=image_inputs,\n    videos=video_inputs,\n    return_tensors=\"pt\",\n    padding=True,\n)\n\n# === Move inputs to GPU and cast to long ===\ninputs = {k: v.to(device) for k, v in inputs.items()}\ninputs[\"input_ids\"] = inputs[\"input_ids\"].long()\nif \"attention_mask\" in inputs:\n    inputs[\"attention_mask\"] = inputs[\"attention_mask\"].long()\n\n# === Inference ===\nwith torch.no_grad():\n    print(\"🧠 Generating explanation...\")\n    generated_ids = model2.generate(**inputs, max_new_tokens=256)\n\n# === Decode output ===\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs[\"input_ids\"], generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\n\n# === Final Output ===\nprint(\"📢 Final Output:\")\nprint(output_text[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}